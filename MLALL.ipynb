{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLALL.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"yji3pQauSmd2","colab_type":"code","colab":{}},"source":["import numpy as np \n","import re\n","# data processing\n","import pandas as pd \n","# data visualization\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from matplotlib import style\n","# Algorithms\n","from sklearn.cluster import KMeans\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","import numpy as np\n","from sklearn import linear_model\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import Perceptron\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import cross_val_predict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9EpvEsFFLd4","colab_type":"code","colab":{}},"source":["#Importing the Libraries\n","# linear algebra\n","import numpy as np \n","\n","# data processing\n","import pandas as pd \n","\n","# data visualization\n","import seaborn as sns\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","from matplotlib import style\n","\n","# Algorithms\n","from sklearn import linear_model\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import Perceptron\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.naive_bayes import GaussianNB\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5qnkeO8FRD8","colab_type":"code","colab":{}},"source":["#Getting the Data\n","\n","test_df = pd.read_csv(\"/content/test.csv\")\n","train_df = pd.read_csv(\"/content/train.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VhYk-fcFZ7K","colab_type":"code","outputId":"de526e79-34a7-4a4c-a9a2-12112d3b1ed5","colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["#Data Exploration/Analysis\n","'''\n","\n","survival:    Survival \n","PassengerId: Unique Id of a passenger. \n","pclass:    Ticket class     \n","sex:    Sex     \n","Age:    Age in years     \n","sibsp:    # of siblings / spouses aboard the Titanic     \n","parch:    # of parents / children aboard the Titanic     \n","ticket:    Ticket number     \n","fare:    Passenger fare     \n","cabin:    Cabin number     \n","embarked:    Port of Embarkation\n","'''\n","\n","train_df.info()\n","train_df.describe()\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 12 columns):\n","PassengerId    891 non-null int64\n","Survived       891 non-null int64\n","Pclass         891 non-null int64\n","Name           891 non-null object\n","Sex            891 non-null object\n","Age            714 non-null float64\n","SibSp          891 non-null int64\n","Parch          891 non-null int64\n","Ticket         891 non-null object\n","Fare           891 non-null float64\n","Cabin          204 non-null object\n","Embarked       889 non-null object\n","dtypes: float64(2), int64(5), object(5)\n","memory usage: 83.7+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Fare</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>714.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>446.000000</td>\n","      <td>0.383838</td>\n","      <td>2.308642</td>\n","      <td>29.699118</td>\n","      <td>0.523008</td>\n","      <td>0.381594</td>\n","      <td>32.204208</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>257.353842</td>\n","      <td>0.486592</td>\n","      <td>0.836071</td>\n","      <td>14.526497</td>\n","      <td>1.102743</td>\n","      <td>0.806057</td>\n","      <td>49.693429</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.420000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>223.500000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>20.125000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>7.910400</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>446.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>28.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>14.454200</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>668.500000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>38.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>31.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>891.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>80.000000</td>\n","      <td>8.000000</td>\n","      <td>6.000000</td>\n","      <td>512.329200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n","count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n","mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n","std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n","min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n","25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n","50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n","75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n","max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n","\n","[8 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"o5yEuTTEHISL","colab_type":"text"},"source":["Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the ‘Age’ feature.\n","We need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with."]},{"cell_type":"code","metadata":{"id":"UYujVF4IHKrM","colab_type":"code","outputId":"86e67b4c-8deb-453a-b22d-ede29cefb7ae","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["total = train_df.isnull().sum().sort_values(ascending=False)\n","percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n","percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n","missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n","missing_data.head(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Total</th>\n","      <th>%</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Cabin</th>\n","      <td>687</td>\n","      <td>77.1</td>\n","    </tr>\n","    <tr>\n","      <th>Age</th>\n","      <td>177</td>\n","      <td>19.9</td>\n","    </tr>\n","    <tr>\n","      <th>Embarked</th>\n","      <td>2</td>\n","      <td>0.2</td>\n","    </tr>\n","    <tr>\n","      <th>Fare</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>Ticket</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Total     %\n","Cabin       687  77.1\n","Age         177  19.9\n","Embarked      2   0.2\n","Fare          0   0.0\n","Ticket        0   0.0"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"gj0ck7vjHmXs","colab_type":"text"},"source":["The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the ‘Age’ feature, which has 177 missing values. The ‘Cabin’ feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing."]},{"cell_type":"markdown","metadata":{"id":"4rEJ6Gu2IRR7","colab_type":"text"},"source":["First, I will drop ‘PassengerId’ from the train set, because it does not contribute to a persons survival probability. I will not drop it from the test set, since it is required there for the submission."]},{"cell_type":"code","metadata":{"id":"9HZy2zOsIQ3v","colab_type":"code","colab":{}},"source":["train_df = train_df.drop(['PassengerId'], axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P74BgiIFId5q","colab_type":"text"},"source":["\n","Missing Data:\n","Cabin:\n","As a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). First I thought, we have to delete the ‘Cabin’ variable but then I found something interesting. A cabin number looks like ‘C123’ and the letter refers to the deck. Therefore we’re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. \n","\n"]},{"cell_type":"code","metadata":{"id":"Dj1QSxnQIN4Y","colab_type":"code","colab":{}},"source":["import re\n","deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n","data = [train_df, test_df]\n","\n","for dataset in data:\n","    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n","    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n","    dataset['Deck'] = dataset['Deck'].map(deck)\n","    dataset['Deck'] = dataset['Deck'].fillna(0)\n","    dataset['Deck'] = dataset['Deck'].astype(int)\n","# we can now drop the cabin feature\n","train_df = train_df.drop(['Cabin'], axis=1)\n","test_df = test_df.drop(['Cabin'], axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zAyQAk6OJMKi","colab_type":"text"},"source":["Age:\n","Now we can tackle the issue with the age features missing values. I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.\n"]},{"cell_type":"code","metadata":{"id":"i--1Hf3VJLqZ","colab_type":"code","outputId":"990d2aec-ea8c-42b1-99bd-c998230d3755","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data = [train_df, test_df]\n","\n","for dataset in data:\n","    mean = train_df[\"Age\"].mean()\n","    std = test_df[\"Age\"].std()\n","    is_null = dataset[\"Age\"].isnull().sum()\n","    # compute random numbers between the mean, std and is_null\n","    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n","    # fill NaN values in Age column with random values generated\n","    age_slice = dataset[\"Age\"].copy()\n","    age_slice[np.isnan(age_slice)] = rand_age\n","    dataset[\"Age\"] = age_slice\n","    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n","train_df[\"Age\"].isnull().sum()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"qfd2E8VrJbdq","colab_type":"text"},"source":["Since the Embarked feature has only 2 missing values, we will just fill these with the most common one."]},{"cell_type":"code","metadata":{"id":"PK099YSJJYtS","colab_type":"code","outputId":"f5c74d0b-b5b7-4ae9-852a-9bc1eb506c7f","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["train_df['Embarked'].describe()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count     889\n","unique      3\n","top         S\n","freq      644\n","Name: Embarked, dtype: object"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"UYyTJdwUJi6i","colab_type":"code","colab":{}},"source":["common_value = 'S'\n","data = [train_df, test_df]\n","\n","for dataset in data:\n","    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5wAYEwHJvp8","colab_type":"text"},"source":["Converting “Fare” from float to int64, using the “astype()” function pandas provides:"]},{"cell_type":"code","metadata":{"id":"vH2gB6GEJul9","colab_type":"code","colab":{}},"source":["data = [train_df, test_df]\n","\n","for dataset in data:\n","    dataset['Fare'] = dataset['Fare'].fillna(0)\n","    dataset['Fare'] = dataset['Fare'].astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucITw9USJ30H","colab_type":"text"},"source":["We will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that."]},{"cell_type":"code","metadata":{"id":"xE5H8ELzJ6jo","colab_type":"code","colab":{}},"source":["data = [train_df, test_df]\n","titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n","\n","for dataset in data:\n","    # extract titles\n","    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n","    # replace titles with a more common title or as Rare\n","    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n","                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n","    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n","    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n","    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","    # convert titles into numbers\n","    dataset['Title'] = dataset['Title'].map(titles)\n","    # filling NaN with 0, to get safe\n","    dataset['Title'] = dataset['Title'].fillna(0)\n","train_df = train_df.drop(['Name'], axis=1)\n","test_df = test_df.drop(['Name'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vh8DI_ufJ_Dt","colab_type":"text"},"source":["Convert ‘Sex’ feature into numeric.\n","genders = {\"male\": 0, \"female\": 1}"]},{"cell_type":"code","metadata":{"id":"QR21hDq6J_66","colab_type":"code","colab":{}},"source":["genders = {\"male\": 0, \"female\": 1}\n","data = [train_df, test_df]\n","\n","for dataset in data:\n","    dataset['Sex'] = dataset['Sex'].map(genders)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Df52R7zhNOGy","colab_type":"code","outputId":"2edcc3d4-7d2a-48af-f6b5-7526ff38d9ef","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["train_df['Ticket'].describe()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count        891\n","unique       681\n","top       347082\n","freq           7\n","Name: Ticket, dtype: object"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"fQ9nJJaENKqq","colab_type":"text"},"source":["Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset."]},{"cell_type":"code","metadata":{"id":"2TxPyHRCNQPK","colab_type":"code","colab":{}},"source":["train_df = train_df.drop(['Ticket'], axis=1)\n","test_df = test_df.drop(['Ticket'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JpYyE5XdNZKb","colab_type":"text"},"source":["Convert ‘Embarked’ feature into numeric"]},{"cell_type":"code","metadata":{"id":"s1--CV_vNW4B","colab_type":"code","colab":{}},"source":["ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n","data = [train_df, test_df]\n","\n","for dataset in data:\n","    dataset['Embarked'] = dataset['Embarked'].map(ports)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9vS29EmENoYU","colab_type":"text"},"source":["Creating Categories:\n","We will now create categories within the following features:\n","Age:\n","Now we need to convert the ‘age’ feature. First we will convert it from float into integer. Then we will create the new ‘AgeGroup” variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don’t want for example that 80% of your data falls into group 1."]},{"cell_type":"code","metadata":{"id":"EMV_fmzlNwIt","colab_type":"code","outputId":"bab09fd7-8c35-432c-f393-b5a35b602c75","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["data = [train_df, test_df]\n","for dataset in data:\n","    dataset['Age'] = dataset['Age'].astype(int)\n","    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n","    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n","    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n","    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n","    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n","    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n","    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n","    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n","\n","# let's see how it's distributed \n","train_df['Age'].value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    891\n","Name: Age, dtype: int64"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"p6nEnULwO24H","colab_type":"text"},"source":["For the ‘Fare’ feature, we need to do the same as with the ‘Age’ feature. But it isn’t that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn “qcut()” function, that we can use to see, how we can form the categories."]},{"cell_type":"code","metadata":{"id":"s565agGJOsyN","colab_type":"code","colab":{}},"source":["\n","data = [train_df, test_df]\n","\n","for dataset in data:\n","    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n","    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n","    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n","    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n","    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n","    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n","    dataset['Fare'] = dataset['Fare'].astype(int)\n","train_df = train_df.drop(['Embarked'], axis=1)\n","test_df = test_df.drop(['Embarked'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"axq4pu6jPJoL","colab_type":"code","outputId":"40e3755f-e0c2-4603-f312-a9b67c935785","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","#Split the data into independent 'X' and dependent 'Y' variables\n","X = np.array(train_df.drop(['Survived'], 1).astype(float))\n","y = np.array(train_df['Survived'])\n","kmeans = KMeans(n_clusters=2) # You want cluster the passenger records into 2: Survived or Not survived\n","kmeans.fit(X)\n","KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n","    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n","    random_state=None, tol=0.0001, verbose=0)\n","correct = 0\n","for i in range(len(X)):\n","    predict_me = np.array(X[i].astype(float))\n","    predict_me = predict_me.reshape(-1, len(predict_me))\n","    prediction = kmeans.predict(predict_me)\n","    if prediction[0] == y[i]:\n","        correct += 1\n","\n","print(correct/len(X))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.6891133557800224\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"aa9a1478-d633-49d9-94ac-fa81b87f2546","id":"Qp7m3rkAV0l8","colab":{"base_uri":"https://localhost:8080/","height":734}},"source":["########################### 1Support Vector Machine  ####################### \n","X_train = train_df.drop(\"Survived\", axis=1)\n","Y_train = train_df[\"Survived\"]\n","X_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n","svm_clf1 = SVC(kernel='linear', C=1)\n","svm_clf1.fit(X_train, Y_train)\n","Y_pred = svm_clf1.predict(X_test)\n","\n","acc_linear_svc1 = round(svm_clf1.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(svm_clf1, X_train, Y_train, cv=3)\n","precision_svc1= round(precision_score(Y_train, Y_pred) * 100, 2)\n","recall_svc1 = round(recall_score(Y_train, Y_pred) * 100, 2)\n","f1_svc1 = round(f1_score(Y_train, Y_pred) * 100, 2)\n","\n","#####################2\n","svm_clf2 = SVC(kernel='linear', C=10)\n","svm_clf2.fit(X_train, Y_train)\n","Y_pred = svm_clf2.predict(X_test)\n","acc_linear_svc2 = round(svm_clf2.score(X_train, Y_train) * 100, 2)\n","\n","Y_pred = cross_val_predict(svm_clf2, X_train, Y_train, cv=3)\n","precision_svc2= round(precision_score(Y_train, Y_pred) * 100, 2)\n","recall_svc2 = round(recall_score(Y_train, Y_pred) * 100, 2)\n","f1_svc2 = round(f1_score(Y_train, Y_pred) * 100, 2)\n","########################3\n","poly_kernel_svm_clf = SVC(kernel=\"poly\", degree=3, coef0=1, C=5)\n"," \n","poly_kernel_svm_clf.fit(X_train, Y_train)\n","acc_linear_svc3 = round(poly_kernel_svm_clf.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(poly_kernel_svm_clf, X_train, Y_train, cv=3)\n","precision_poly= round(precision_score(Y_train, Y_pred) * 100, 2)\n","recall_poly = round(recall_score(Y_train, Y_pred) * 100, 2)\n","f1_poly = round(f1_score(Y_train, Y_pred) * 100, 2)\n","##########################4\n","rbf_kernel_svm_clf = SVC(kernel=\"rbf\", gamma=0.1, C=1000)\n","rbf_kernel_svm_clf.fit(X_train, Y_train)\n","\n","acc_linear_svc4 = round(rbf_kernel_svm_clf.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(rbf_kernel_svm_clf , X_train, Y_train, cv=3)\n","precision_rbf=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_rbf=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_rbf = round(f1_score(Y_train, Y_pred) * 100, 2)\n","##########################5 logistic regression ###################\n","logreg = LogisticRegression()\n","logreg.fit(X_train, Y_train)\n","\n","acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(logreg , X_train, Y_train, cv=3)\n","precision_lg=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_lg=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_lg = round(f1_score(Y_train, Y_pred) * 100, 2)\n","###########################6 naive Bayes ######################## \n","gaussian = GaussianNB()\n","gaussian.fit(X_train, Y_train) \n","\n","acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(gaussian , X_train, Y_train, cv=3)\n","precision_NB=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_NB=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_NB = round(f1_score(Y_train, Y_pred) * 100, 2)\n","####################### 7decision tree ###################\n","decision_tree1 = DecisionTreeClassifier() \n","decision_tree1.fit(X_train, Y_train)\n","\n","acc_decision_tree1 = round(decision_tree1.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(decision_tree1 , X_train, Y_train, cv=3)\n","precision_DT=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_DT=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_DT = round(f1_score(Y_train, Y_pred) * 100, 2)\n","##########################8\n","decision_tree2 = DecisionTreeClassifier(max_depth=3, min_samples_split=0.1, min_samples_leaf=0.1) \n","decision_tree2.fit(X_train, Y_train)\n","   \n","acc_decision_tree2 = round(decision_tree2.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(decision_tree2 , X_train, Y_train, cv=3)\n","precision_DT2=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_DT2=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_DT2 = round(f1_score(Y_train, Y_pred) * 100, 2)\n","###########################9\n","decision_tree3 = DecisionTreeClassifier(max_depth=10, min_samples_split=3,min_samples_leaf=5) \n","decision_tree3.fit(X_train, Y_train)\n","\n","acc_decision_tree3 = round(decision_tree3.score(X_train, Y_train) * 100, 2)\n","Y_pred = cross_val_predict(decision_tree3 , X_train, Y_train, cv=3)\n","precision_DT3=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_DT3=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_DT3 = round(f1_score(Y_train, Y_pred) * 100, 2)\n","############################ 10 Random forest #######################\n","random_forest = RandomForestClassifier(criterion = \"gini\", \n","                                       max_depth= 6,\n","                                       n_estimators=100, \n","                                       max_features='log2', \n","                                       )\n","\n","random_forest.fit(X_train, Y_train)\n","\n","random_forest_score=(round(random_forest.score(X_train, Y_train)* 100,2))\n","Y_pred = cross_val_predict(random_forest , X_train, Y_train, cv=3)\n","precision_rf=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_rf=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_rf = round(f1_score(Y_train, Y_pred) * 100, 2)\n","################### 11Gradiant boosting with grid search cv #####################\n","\n","\n","gradientBoosting = GradientBoostingClassifier(min_samples_leaf= 40, n_estimators= 20, min_samples_split= 200, learning_rate= 1, max_depth= 7) \n","                                       \n","\n","gradientBoosting.fit(X_train, Y_train)\n","\n","GB_score=(round(gradientBoosting.score(X_train, Y_train)* 100,2))\n","Y_pred = cross_val_predict(gradientBoosting , X_train, Y_train, cv=3)\n","precision_gb=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_gb=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_gb = round(f1_score(Y_train, Y_pred) * 100, 2)\n","##################12 Bagging classifier Decision tree  ##################\n","bag_clf_DT=BaggingClassifier(DecisionTreeClassifier(),n_estimators=100,max_samples=100,bootstrap=True,n_jobs=-1)\n","bag_clf_DT.fit(X_train, Y_train)\n","\n","bag_clf_Score_DT=(round(bag_clf_DT.score(X_train, Y_train)* 100,2))\n","Y_pred = cross_val_predict(bag_clf_DT , X_train, Y_train, cv=3)\n","precision_bagg_dt=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_bagg_dt=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_bagg_dt = round(f1_score(Y_train, Y_pred) * 100, 2)\n","##################### 13Bagging naive bayes ###################\n","bag_clf_NB=BaggingClassifier(GaussianNB() ,n_estimators=100,max_samples=100,bootstrap=True,n_jobs=-1)\n","bag_clf_NB.fit(X_train, Y_train)\n","\n","bag_clf_Score_NB=(round(bag_clf_NB.score(X_train, Y_train)* 100,2))\n","Y_pred = cross_val_predict(bag_clf_NB , X_train, Y_train, cv=3)\n","precision_bagg_nb=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_bagg_nb=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_bagg_nb = round(f1_score(Y_train, Y_pred) * 100, 2)\n","####################14 Ada boosting #########################\n","Ada_boost = AdaBoostClassifier( n_estimators= 400, learning_rate= 0.05, algorithm= 'SAMME.R')                         \n","Ada_boost.fit(X_train, Y_train)\n","\n","Ada_score=(round(Ada_boost.score(X_train, Y_train)* 100,2))\n","Y_pred = cross_val_predict(Ada_boost , X_train, Y_train, cv=3)\n","precision_ada_boost=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_ada_boost=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_ada_boost = round(f1_score(Y_train, Y_pred) * 100, 2)\n","##################################15 Voting ##########################\n","voting_clf = VotingClassifier(\n"," estimators=[('lr', logreg), ('rf', random_forest), ('svc', svm_clf2), ('DT',decision_tree1),('NB',gaussian)],\n"," voting='hard'\n"," )\n","voting_clf.fit(X_train, Y_train)\n","\n","voting_score=(round(voting_clf.score(X_train, Y_train)* 100,2))\n","Y_pred = cross_val_predict(voting_clf , X_train, Y_train, cv=3)\n","precision_voting=round(precision_score(Y_train,Y_pred)* 100, 2)\n","recall_voting=round(recall_score(Y_train,Y_pred)* 100, 2)\n","f1_voting = round(f1_score(Y_train, Y_pred) * 100, 2)\n","########################################## Result #################\n","results = pd.DataFrame({\n","    'Model': ['SVM  linear c=1','svm linear c=10 ','svm poly kernel', 'svm rbf kernel', 'Logistic Regression', \n","               'Naive Bayes', \n","              'Decision Tree1','Decision Tree2','Decision tree3','random_forest_score','GB_score','bag_clf_Score_DT','bag_clf_Score_NB','Ada_boost','Voting'],\n","    'Score': [acc_linear_svc1, acc_linear_svc2,acc_linear_svc3, acc_linear_svc4, acc_log, \n","             acc_gaussian, \n","              acc_decision_tree1,acc_decision_tree2,acc_decision_tree3,random_forest_score,GB_score,bag_clf_Score_DT,bag_clf_Score_NB,Ada_score,voting_score],\n","              'Precision_Score':[precision_svc1, precision_svc2,precision_poly,precision_rbf,precision_lg,precision_NB,precision_DT,precision_DT2,precision_DT3,precision_rf,precision_gb,precision_bagg_dt,precision_bagg_nb,precision_ada_boost,precision_voting], \n","              'Recall_Score':[recall_svc1, recall_svc2,recall_poly,recall_rbf,recall_lg,recall_NB,recall_DT,recall_DT2,recall_DT3,recall_rf,recall_gb,recall_bagg_dt,recall_bagg_nb,recall_ada_boost,recall_voting],\n","              'F1_Score':[f1_svc1, f1_svc2,f1_poly,f1_rbf,f1_lg,f1_NB,f1_DT,f1_DT2,f1_DT3,f1_rf,f1_gb,f1_bagg_dt,f1_bagg_nb,f1_ada_boost,f1_voting]\n","               }\n","              )\n","              \n","              \n","print(results)       \n","'''results.plot(x= 'Model', kind='barh')\n","plt.title('Models Performance')\n","plt.show()'''\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["                  Model  Score  Precision_Score  Recall_Score  F1_Score\n","0       SVM  linear c=1  78.45            74.84         69.59     72.12\n","1      svm linear c=10   78.45            74.84         69.59     72.12\n","2       svm poly kernel  87.88            79.29         71.64     75.27\n","3        svm rbf kernel  92.70            73.46         66.37     69.74\n","4   Logistic Regression  81.03            74.56         73.68     74.12\n","5           Naive Bayes  78.23            67.92         79.24     73.14\n","6        Decision Tree1  93.04            76.59         66.96     71.45\n","7        Decision Tree2  78.90            77.10         59.06     66.89\n","8        Decision tree3  86.53            78.26         68.42     73.01\n","9   random_forest_score  86.98            81.33         71.35     76.01\n","10             GB_score  87.88            82.99         71.35     76.73\n","11     bag_clf_Score_DT  86.42            82.99         69.88     75.87\n","12     bag_clf_Score_NB  78.34            66.75         76.32     71.21\n","13            Ada_boost  83.28            76.72         75.15     75.92\n","14               Voting  83.84            77.20         74.27     75.71\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["\"results.plot(x= 'Model', kind='barh')\\nplt.title('Models Performance')\\nplt.show()\""]},"metadata":{"tags":[]},"execution_count":58}]}]}